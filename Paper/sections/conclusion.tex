Having detailed the dataset construction, evaluation, and fine-tuning analysis, we now synthesize the main takeaways.

\subsection{Conclusion}
We introduced \textit{RecencyQA-Multi}, a systematically generated extension of the original RecencyQA corpus that explicitly varies stationarity, the number of interdependent events, and the relationships between them (Section~\ref{sec:dataset-creation}). Starting from only \num{75} seed questions, our controlled prompting and verification pipeline yielded \num{1411} questions with fine-grained recency labels and contextual conditions, providing a richer testbed for studying temporal reasoning.

To gauge how well current models exploit this structure, we designed a reusable evaluation pipeline (Section~\ref{sec:testing-pipeline}) and benchmarked state-of-the-art instruction models (Section~\ref{sec:results}). Despite tolerant accuracies exceeding \num{70}\unit{\%}, all models misclassify most instances under the strict metric, and accuracy drops sharply for non-stationary and multi-event questions. Qwen2.5-72B performs best overall, yet still struggles to distinguish medium-horizon labels and consistently predicts overly fresh answers, underscoring the difficulty of calibrating temporal priors even for large LLMs.

Fine-tuning a smaller model confirms that adaptation can close much of the scale gap: the \textit{Qwen2.5-14B-Instruct-recency} model reaches \num{40.9}\unit{\%} accuracy and \num{69.6}\unit{\%} tolerant F$_1$ on the reduced test set, matching DeepSeek-V3 and surpassing Kimi-K2 while trailing Qwen2.5-72B. When normalized by parameter count, the fine-tuned model delivers the strongest performance per parameter (Figure~\ref{fig:finetune-param-efficiency}), underscoring that targeted fine-tuning yields substantial efficiency gains.

\subsection{Future Research}
Several directions for future work could strengthen both the reliability and expressiveness of recency labels. A natural next step is to construct a dataset in which labels are assigned by humans at scale, ideally with multiple independent annotators per question. This could be facilitated through a lightweight web interface, and the final label could be derived from an aggregation rule such as the median or majority vote to reduce individual bias and noise. This would allow a more accurate evaluation of model performance against human judgment.

In addition, model ensembles offer a complementary avenue: multiple models could be run in parallel and the most consistently predicted label could be used as the final decision. In cases where the ensemble remains inconclusive, additional models (or a second-stage decision process) could be introduced, informed by the earlier model outputs to guide disambiguation. Potentially, this could yield more robust recency estimates by leveraging diverse model perspectives.

Finally, instead of treating recency prediction as a single hard-label task, it may be more appropriate to model ambiguity explicitly by using a distribution over labels as the target. For example, if annotators disagree between \enquote{A-Week} and \enquote{A-Month}, the training signal could reflect this uncertainty via proportional target probabilities. Such soft targets would be less brittle than fixed labels and could better capture the inherently fuzzy boundary between time ranges.
