Large language models (LLMs) answer questions using knowledge that may change over time; however, most evaluations remain static. This work explores the demands of question recency, specifically the frequency with which answers must be updated to maintain accuracy, and the precision with which models can predict the necessary update frequency. 
We construct a \textit{generation-and-labeling} pipeline that produces recency-aware questions along two axes: stationary vs.versus non-stationary temporal behaviour, and single-event vs. multi-event dependencies. The multi-event category captures questions whose answers rely on multiple temporally connected events, including causal and purely co-occurring pairs. The single-event category focuses on questions about a single evolving process. Stationarity labels distinguish predictable update cycles from volatile changes.
Employing a Llama model, the pipeline generates and labels a $\sim$\num{1400}-question dataset, assigning recency classes, stationarity labels, and multi-event markers. Subsequently, it applies perturbations to construct more challenging temporal variants. The resulting dataset extends prior work, providing new and more demanding questions with which to evaluate LLMs on when those answers change.