In this section we describe our testing pipeline for evaluating the performance of language models on our dataset, or on custom datasets. Thus, our pipeline is designed to be usable for any dataset that follows this structure and any LLM that can be accessed via \emph{Together AI}\footnote{One could also use other platforms or options than Together AI, but this would need a small refactoring of how to access and execute those models}. The pipeline consumes the unmodified \texttt{JSON} produced by the generation process (see Appendix~\ref{app:sec:output-json}).

After loading the dataset, we iterate over every question and unpack its list of temporal contexts and gold labels as provided by the schema in Appendix~\ref{app:sec:output-json}. Each entry becomes an independent evaluation instance consisting of a question, one context sentence, the corresponding recency label, and metadata describing stationarity, event dependency, and the number of events. This flattening step allows us to handle questions with one or multiple applicable contexts uniformly, while retaining the ability to later aggregate results by stationary vs.
non-stationary behavior or by the structural class of the question.

For every instance we assemble the testing prompt shown in Appendix~\ref{app:testing-prompts} and submit it to the selected model via Together AI. The prompt enforces single-label answers from the same discrete label set that was used during generation, ensuring direct comparability between model predictions and the human-authored ground truth. The temperature is fixed at \num{0,0} to minimize randomness and creativity in the outputs.

The pipeline writes one \texttt{JSONL} file per model containing all predictions, including the original question, the used context, the gold label, and the predicted label. In addition, it produces a compact summary \texttt{JSON} that reports accuracy, a tolerant accuracy (+/- one label), the number of evaluated instances, and the count of invalid responses for each model. These metrics are computed both globally and for slices such as stationary vs. non-stationary or single- vs. multi-event questions, enabling quick inspection of where a model struggles. 
